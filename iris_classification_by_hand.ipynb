{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a9aed070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7288cf",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c0a1c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8b3012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(theta, X, y):\n",
    "    m = y.size      # This is the number of instances\n",
    "    return (X.T @ (sigmoid(X @ theta) - y)) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7e023877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha=0.1, num_iterations=100, tol=1e-7):\n",
    "    X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "    theta = np.zeros(X_bias.shape[1])\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradient = calculate_gradient(theta, X_bias, y)\n",
    "        theta -= alpha * gradient\n",
    "\n",
    "        if np.linalg.norm(gradient) < tol:\n",
    "            break\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "627758f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, theta):\n",
    "    X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "    return sigmoid(X_bias @ theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3ec00b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta, threshold=0.5):\n",
    "    return (predict_proba(X, theta) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cda635f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f6e88efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "X, y = iris_data.data, iris_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "26ac2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "35f7e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3 separate binary classifiers\n",
      "    Training classifier for class 0 (setosa) vs the rest\n",
      "    Training classifier for class 1 (versicolor) vs the rest\n",
      "    Training classifier for class 2 (virginica) vs the rest\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Training one classifier for each class using the One Vs Rest method (class 0 or non-class 0 and so on)\n",
    "all_thetas = []\n",
    "num_classes = len(np.unique(y))     # There are 3 unique classes in the Iris dataset\n",
    "\n",
    "print(f\"Training {num_classes} separate binary classifiers\")\n",
    "\n",
    "for i in range(num_classes):\n",
    "    # Create a temporary target vector for this specific class\n",
    "    y_train_specific = (y_train == i).astype(int)\n",
    "\n",
    "    print(f\"    Training classifier for class {i} ({iris_data.target_names[i]}) vs the rest\")\n",
    "\n",
    "    # Using the gradient descent function to train the binary classifier\n",
    "    theta_hat = gradient_descent(X_train_scaled, y_train_specific, alpha=0.1, num_iterations=1000)\n",
    "\n",
    "    # Store the trained parameters\n",
    "    all_thetas.append(theta_hat)\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c05ff0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ovr(X, thetas):\n",
    "    probabilities = np.array([predict_proba(X, theta) for theta in thetas]).T\n",
    "\n",
    "    return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1964719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 0.9667\n",
      "\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.89      0.94         9\n",
      "   virginica       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on the test set\n",
    "y_pred = predict_ovr(X_test_scaled, all_thetas)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report for a more detailed look\n",
    "print(\"\\nClassification report\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c63277",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (KNN) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c0d2947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b45d46ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euclidean_dist(p, q):\n",
    "    # This is a np.array of every value in p - every value in q\n",
    "    #np.array(p) - np.array(q)\n",
    "\n",
    "    return np.sqrt(np.sum((np.array(p) - np.array(q)) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "da54ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNeighborsClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_trian, y_train):\n",
    "        self.X_train = X_trian\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict_single(self, x_test_point):\n",
    "        '''Helper function to predict the label of one single data point'''\n",
    "\n",
    "        # Calculate the distance to all training points\n",
    "        distances = [calc_euclidean_dist(x_test_point, x_train_point) for x_train_point in self.X_train]\n",
    "\n",
    "        # Get the indices of the k nearest neighbors\n",
    "        k_neigbors_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # Get the labels of the neighbors\n",
    "        k_neigbors_labels = [self.y_train[i] for i in k_neigbors_indices]\n",
    "\n",
    "        # Return the most common class label \n",
    "        most_common = Counter(k_neigbors_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        '''Makes predictions for a set of test data'''\n",
    "        \n",
    "        # Loop through each test point and predict its label\n",
    "        preds = [self.predict_single(x_test_point) for x_test_point in X_test]\n",
    "        return np.array(preds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "82cc4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preparing the data\n",
    "iris_data = load_iris()\n",
    "X, y = iris_data.data, iris_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scld = scaler.fit_transform(X_train)\n",
    "X_test_scld = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "37ae551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(k=5)\n",
    "\n",
    "knn_classifier.fit(X_train_scld, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_predictions = knn_classifier.predict(X_test_scld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b8345637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model accuracy: 0.9667\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00         8\n",
      "  versicolor       1.00      0.91      0.95        11\n",
      "   virginica       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "acc = accuracy_score(y_test, y_predictions)\n",
    "\n",
    "print(f\"KNN model accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_predictions, target_names=iris_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b3abb",
   "metadata": {},
   "source": [
    "## Decision Trees Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a26d960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bb9825de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        # Check the stopping criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_features, self.n_features, replace=False)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # Create child nodes \n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # Calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # Parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # Create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # Calculate the weighted average entropy of children\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(left_idxs), len(right_idxs)\n",
    "        entropy_left, entropy_right = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_left / n) * entropy_left + (n_right / n) * entropy_right\n",
    "\n",
    "        # Calculate the information gain\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_threshold):\n",
    "        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        # This is similar to a freq vect\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "\n",
    "        return -np.sum([p * np.log(p) for p in ps if p > 0])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3e469588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use decision tree classifier\n",
    "decision_tree_classifier = DecisionTree(max_depth=3)\n",
    "\n",
    "decision_tree_classifier.fit(X_train_scld, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "dtree_y_predictions = decision_tree_classifier.predict(X_test_scld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa52cf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model accuracy: 0.9667\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00         8\n",
      "  versicolor       1.00      0.82      0.90        11\n",
      "   virginica       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.95      0.94      0.94        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "dtree_accuracy = accuracy_score(y_test, dtree_y_predictions)\n",
    "\n",
    "print(f\"Decision Tree model accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, dtree_y_predictions, target_names=iris_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc284a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Iris_Classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
